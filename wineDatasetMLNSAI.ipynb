{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Loss: 0.2849\n",
      "Epoch [10/50], Loss: 0.0019\n",
      "Epoch [20/50], Loss: 0.0064\n",
      "Epoch [30/50], Loss: 0.0002\n",
      "Epoch [40/50], Loss: 0.0000\n",
      "Test Accuracy: 99.77%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 1. Data Preprocessing (Level 1)\n",
    "data = pd.read_csv('wine_dataset.csv')\n",
    "data = data.apply(pd.to_numeric, errors='coerce', downcast='float', axis=1)\n",
    "\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values   # Target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Define the Multi-Level Neuro-Symbolic AI Model\n",
    "class MultiLevelNeuroSymbolicModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3):\n",
    "        super(MultiLevelNeuroSymbolicModel, self).__init__()\n",
    "        # Level 1: Neural network for feature extraction\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Level 2: Symbolic reasoning (abstracting features to symbolic forms)\n",
    "        self.symbolic_layer = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Level 3: Integration of external knowledge - Rule-based system\n",
    "        # Example rule-based knowledge about alcohol content (we can extend it with external knowledge)\n",
    "        self.alcohol_threshold = 12.0  # Define a threshold for wine classification (hypothetical)\n",
    "        \n",
    "        # Level 4: Meta-learning placeholder (meta-learning layer, simulated with a MAML-like layer)\n",
    "        self.meta_layer = nn.Linear(hidden_size3, hidden_size3)\n",
    "\n",
    "        # Level 5: Fairness Constraints in output generation (to be added)\n",
    "        self.fc_final = nn.Linear(hidden_size3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Level 1: Neural feature extraction\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "\n",
    "        # Level 2: Symbolic reasoning layer\n",
    "        x = self.relu3(self.symbolic_layer(x))\n",
    "\n",
    "        # Level 3: Knowledge integration - Apply basic rule-based external knowledge (hypothetical)\n",
    "        # Example: If the alcohol content is greater than 12, modify the neural outputs\n",
    "        if torch.mean(x[:, 0]) > self.alcohol_threshold:\n",
    "            x[:, 0] += 0.1  # Modify feature representations based on rule\n",
    "\n",
    "        # Level 4: Meta-learning - Simulated MAML-like layer for better generalization\n",
    "        x = self.meta_layer(x)\n",
    "\n",
    "        # Level 5: Fairness constrained output (to be added)\n",
    "        x = self.sigmoid(self.fc_final(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "hidden_size3 = 16\n",
    "model = MultiLevelNeuroSymbolicModel(input_size, hidden_size1, hidden_size2, hidden_size3)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function (Level 1-4 training)\n",
    "def train_model(model, train_loader, num_epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.view(-1, 1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, num_epochs=50)\n",
    "\n",
    "# Test function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions.view(-1) == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Test the model\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  style  \n",
      "0      9.4        5      0  \n",
      "1      9.8        5      0  \n",
      "2      9.8        5      0  \n",
      "3      9.8        6      0  \n",
      "4      9.4        5      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('wine_dataset.csv')\n",
    "\n",
    "# Inspect the first few rows of the dataset to understand its structure\n",
    "print(data.head())\n",
    "\n",
    "# Assuming the last column is the target and the rest are features\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values   # Target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature values (neural networks perform better with scaled data)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Loss: 0.7719\n",
      "Epoch [10/50], Loss: 0.0050\n",
      "Epoch [20/50], Loss: 0.0024\n",
      "Epoch [30/50], Loss: 0.0001\n",
      "Epoch [40/50], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Define the Multi-Level Neuro-Symbolic AI Model\n",
    "class MultiLevelNeuroSymbolicModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3):\n",
    "        super(MultiLevelNeuroSymbolicModel, self).__init__()\n",
    "        # Level 1: Neural network for feature extraction\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Level 2: Refined layer\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # Level 3: Final output layer for binary classification\n",
    "        self.fc4 = nn.Linear(hidden_size3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass data through the neural layers\n",
    "        x = self.relu1(self.fc1(x))  # Level 1\n",
    "        x = self.relu2(self.fc2(x))  # Level 2\n",
    "        x = self.relu3(self.fc3(x))  # Level 3\n",
    "        x = self.sigmoid(self.fc4(x))  # Output\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "hidden_size3 = 16\n",
    "model = MultiLevelNeuroSymbolicModel(input_size, hidden_size1, hidden_size2, hidden_size3)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, num_epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.view(-1, 1))\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Training the model\n",
    "train_model(model, train_loader, num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.46%\n"
     ]
    }
   ],
   "source": [
    "# Example symbolic reasoning rule\n",
    "def apply_symbolic_rules(predictions):\n",
    "    # Example logic: if the probability is > 0.7, we assume class 1; else, class 0\n",
    "    refined_preds = torch.where(predictions > 0.7, torch.tensor(1.0), torch.tensor(0.0))\n",
    "    return refined_preds\n",
    "\n",
    "# Test the model and apply symbolic reasoning\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predictions = apply_symbolic_rules(outputs)\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions.view(-1) == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Test the model on the test data\n",
    "test_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
